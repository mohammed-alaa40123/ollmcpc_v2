{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to OLLMCPC","text":"<p>OLLMCPC is a high-performance, terminal-based client for the Model Context Protocol (MCP). It acts as a bridge between powerful Large Language Models (LLMs) and your local system environment.</p> <p>High Performance</p> <p>Written in modern C++17, OLLMCPC is designed for speed and minimal resource usage, making it ideal for developers who want a snappy local AI assistant.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Multi-Provider Integration: Support for Ollama (local) and Gemini (cloud).</li> <li>Built-in OS Assistant: A dedicated MCP server that provides tools for filesystem access, process management, and more.</li> <li>Human-in-the-Loop (HIL): Built-in safety mechanism that prompts for user approval before executing system tools.</li> <li>Extensible: Easily connect external MCP servers written in any language (Node.js, Python, Go).</li> <li>Modern TUI: A beautiful terminal interface with colorized output and interactive menus.</li> </ul>"},{"location":"#how-it-works","title":"How it Works","text":"<p>OLLMCPC connects to an LLM provider and one or more MCP servers. When you ask a question, the LLM determines if it needs to use a tool provided by an MCP server. If so, OLLMCPC handles the communication, enforces safety checks, and returns the results to the LLM.</p> <pre><code>graph LR\n    User --&gt;|Prompts| Client[OLLMCPC Client]\n    Client &lt;--&gt;|API| LLM[Ollama / Gemini]\n    Client &lt;--&gt;|stdio| MCP[MCP Servers]\n    MCP --&gt;|Executes| Tools[Local Tools]</code></pre>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Ready to dive in? Check out the Installation guide to get OLLMCPC running on your system.</p>"},{"location":"architecture/","title":"System Architecture","text":"<p>OLLMCPC is built with a modular approach, separating the UI, LLM logic, and protocol handling.</p>"},{"location":"architecture/#overview","title":"Overview","text":"<p>The system consists of three main layers:</p> <ol> <li>Application Layer: Handles the CLI, Terminal UI (TUI), and configuration management.</li> <li>LLM Layer: A set of provider strategies that translate general tool/chat requests into provider-specific API calls (Ollama, Gemini).</li> <li>MCP Layer: Implements the Model Context Protocol. It manages server lifecycles, tool discovery, and execution.</li> </ol>"},{"location":"architecture/#flow-diagram","title":"Flow Diagram","text":"<pre><code>sequenceDiagram\n    participant User\n    participant App as Client (App)\n    participant LLM as LLM Provider\n    participant MCP as MCP Registry\n    participant Srv as MCP Server\n\n    User-&gt;&gt;App: Submits prompt\n    App-&gt;&gt;MCP: Get available tools\n    MCP--&gt;&gt;App: Tool list\n    App-&gt;&gt;LLM: Send message + Tool schemas\n    LLM-&gt;&gt;App: Request tool execution\n    App-&gt;&gt;User: Request HIL Approval?\n    User-&gt;&gt;App: Approved!\n    App-&gt;&gt;MCP: Call tool(params)\n    MCP-&gt;&gt;Srv: Run tool\n    Srv--&gt;&gt;MCP: Tool output\n    MCP--&gt;&gt;App: Result\n    App-&gt;&gt;LLM: Return result to LLM\n    LLM--&gt;&gt;App: Final response\n    App-&gt;&gt;User: Display response</code></pre>"},{"location":"architecture/#key-components","title":"Key Components","text":""},{"location":"architecture/#the-client-app","title":"The Client (App)","text":"<p>The main entry point (<code>main.cpp</code>) handles routing. The interactive loop (<code>interactive.cpp</code>) manages the state of the chat, rendering colors, and processing slash commands.</p>"},{"location":"architecture/#the-mcp-registry","title":"The MCP Registry","text":"<p>The <code>MCPClient</code> class is the central nervous system. It: *   Launches local and external servers. *   Aggregates tools from all connected servers into a single registry. *   Dispatches tool calls to the correct server based on the server prefix.</p>"},{"location":"architecture/#server-proxies","title":"Server Proxies","text":"<p>External servers (like those run via <code>npx</code>) are managed by <code>MCPServerProxy</code>. It handles: *   Process spawning via <code>pipe</code>. *   JSON-RPC communication over <code>stdin</code>/<code>stdout</code>. *   Error handling and logging.</p>"},{"location":"configuration/","title":"Configuration","text":"<p>OLLMCPC configuration is stored in a JSON file at <code>~/.ollmcpc.json</code>.</p>"},{"location":"configuration/#configuration-wizard","title":"Configuration Wizard","text":"<p>You can initialize or update your configuration interactively:</p> <pre><code>./build/ollmcpc config\n</code></pre> <p>This will prompt you for: *   Default LLM provider. *   Ollama model name. *   Gemini API Key and model name. *   Human-in-the-Loop (HIL) toggle.</p>"},{"location":"configuration/#manual-configuration","title":"Manual Configuration","text":"<p>Heres an example of a complete <code>~/.ollmcpc.json</code> file:</p> <pre><code>{\n  \"default_provider\": \"ollama\",\n  \"ollama_model\": \"functiongemma\",\n  \"gemini_api_key\": \"YOUR_API_KEY_HERE\",\n  \"gemini_model\": \"gemini-1.5-flash\",\n  \"human_in_loop\": true,\n  \"servers\": [\n    {\n      \"name\": \"filesystem\",\n      \"command\": [\"npx\", \"-y\", \"@modelcontextprotocol/server-filesystem\", \"/home/user/documents\"],\n      \"enabled\": true\n    },\n    {\n      \"name\": \"web-browsing\",\n      \"command\": [\"npx\", \"-y\", \"mcp-fetch-server\"],\n      \"enabled\": true\n    }\n  ]\n}\n</code></pre>"},{"location":"configuration/#options-explained","title":"Options Explained","text":"Key Type Description <code>default_provider</code> string Either <code>ollama</code>, <code>gemini</code>, or <code>manual</code>. <code>ollama_model</code> string The model name in your local Ollama library. <code>gemini_api_key</code> string Your Google AI Studio API key. <code>gemini_model</code> string The specific Gemini model to use (e.g., <code>gemini-1.5-pro</code>). <code>human_in_loop</code> boolean If true, asks for permission before executing tools. <code>servers</code> array A list of external MCP servers to launch."},{"location":"configuration/#server-configuration","title":"Server Configuration","text":"<p>Each object in the <code>servers</code> array requires: *   <code>name</code>: A unique identifier for the server. *   <code>command</code>: An array of strings representing the command to run (e.g., <code>[\"npx\", \"mcp-server-name\"]</code>). *   <code>enabled</code>: (Optional) If set to <code>false</code>, the server won't be started automatically.</p>"},{"location":"deployment/","title":"Deployment Guide","text":""},{"location":"deployment/#github-pages-deployment","title":"GitHub Pages Deployment","text":"<p>OLLMCPC documentation is automatically deployed to GitHub Pages using GitHub Actions.</p>"},{"location":"deployment/#setup-instructions","title":"Setup Instructions","text":"<ol> <li>Enable GitHub Pages in your repository settings:</li> <li>Go to Settings \u2192 Pages</li> <li>Under Source, select Deploy from a branch</li> <li>Select the <code>gh-pages</code> branch</li> <li> <p>Click Save</p> </li> <li> <p>Push to trigger deployment:    <pre><code>git add .\ngit commit -m \"Add documentation\"\ngit push origin main\n</code></pre></p> </li> <li> <p>Access your docs:    Your documentation will be available at:    <code>https://mohammed-alaa40123.github.io/ollmcpc_v2/</code></p> </li> </ol>"},{"location":"deployment/#how-it-works","title":"How it Works","text":"<p>The GitHub Actions workflow (<code>.github/workflows/docs.yml</code>) automatically: 1. Triggers on every push to <code>main</code> or <code>master</code> branch 2. Sets up Python and installs MkDocs Material 3. Builds the documentation 4. Deploys to the <code>gh-pages</code> branch</p>"},{"location":"deployment/#manual-deployment","title":"Manual Deployment","text":"<p>You can also deploy manually from your local machine:</p> <pre><code># Install dependencies first\npip install mkdocs-material\n\n# Deploy\nmkdocs gh-deploy\n</code></pre> <p>This will build the docs and push them to the <code>gh-pages</code> branch.</p>"},{"location":"deployment/#custom-domain-optional","title":"Custom Domain (Optional)","text":"<p>To use a custom domain: 1. Add a <code>CNAME</code> file to the <code>docs/</code> directory with your domain name 2. Configure your DNS provider to point to GitHub Pages 3. Enable custom domain in repository settings</p>"},{"location":"installation/","title":"Installation Guide","text":"<p>Follow these steps to set up OLLMCPC on your local machine.</p>"},{"location":"installation/#prerequisites","title":"Prerequisites","text":"<p>OLLMCPC requires a Linux environment (Ubuntu or Debian recommended).</p>"},{"location":"installation/#system-packages","title":"System Packages","text":"<p>Install the necessary build tools and libraries:</p> <pre><code>sudo apt-get update\nsudo apt-get install -y \\\n    build-essential \\\n    cmake \\\n    libcurl4-openssl-dev \\\n    libssl-dev \\\n    curl \\\n    nodejs \\\n    npm\n</code></pre>"},{"location":"installation/#llm-backends","title":"LLM Backends","text":"<ul> <li>Ollama (Recommended for local LLM):     Install via their official script:     <pre><code>curl -fsSL https://ollama.com/install.sh | sh\n</code></pre></li> <li>Gemini:     No local installation required, but you will need an API Key from the Google AI Studio.</li> </ul>"},{"location":"installation/#build-from-source","title":"Build from Source","text":"<p>Clone the repository and build using CMake:</p> <pre><code>git clone https://github.com/mohammed-alaa40123/ollmcpc_v2.git\ncd ollmcpc_v2\nmkdir build &amp;&amp; cd build\ncmake ..\nmake -j$(nproc)\n</code></pre> <p>This generates two main binaries in the <code>build/</code> directory: 1.  <code>ollmcpc</code>: The interactive client application. 2.  <code>mcp_server</code>: The internal provider of system tools.</p>"},{"location":"installation/#automated-setup","title":"Automated Setup","text":"<p>For a faster workflow, you can use the provided setup script:</p> <pre><code>chmod +x scripts/setup.sh\n./scripts/setup.sh\n</code></pre> <p>The script will: 1.  Check and install missing system dependencies. 2.  Install Ollama if not present. 3.  Build the project. 4.  Pull the recommended models (<code>functiongemma</code> and <code>qwen3:0.6b</code>). 5.  Create a default configuration file. 6.  Install the binaries to <code>/usr/local/bin</code>.</p>"},{"location":"protocol/","title":"JSON-RPC Protocol &amp; Tool Integration","text":"<p>This document explains how OLLMCPC implements the Model Context Protocol (MCP) using JSON-RPC 2.0, and how tools are integrated with Large Language Models.</p>"},{"location":"protocol/#overview","title":"Overview","text":"<p>OLLMCPC uses JSON-RPC 2.0 as the transport protocol for all MCP communication. This enables: - Bidirectional communication between the client and MCP servers - Standardized request/response patterns - Tool discovery and execution across different server implementations</p>"},{"location":"protocol/#json-rpc-20-basics","title":"JSON-RPC 2.0 Basics","text":""},{"location":"protocol/#message-structure","title":"Message Structure","text":"<p>All JSON-RPC messages are single-line JSON objects terminated by a newline (<code>\\n</code>).</p>"},{"location":"protocol/#request-format","title":"Request Format","text":"<pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"id\": 1,\n  \"method\": \"tools/list\",\n  \"params\": {}\n}\n</code></pre>"},{"location":"protocol/#response-format","title":"Response Format","text":"<pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"id\": 1,\n  \"result\": {\n    \"tools\": [...]\n  }\n}\n</code></pre>"},{"location":"protocol/#notification-format-no-response-expected","title":"Notification Format (No Response Expected)","text":"<pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"method\": \"notifications/initialized\",\n  \"params\": {}\n}\n</code></pre>"},{"location":"protocol/#mcp-server-lifecycle","title":"MCP Server Lifecycle","text":""},{"location":"protocol/#1-connection-establishment","title":"1. Connection Establishment","text":"<p>When OLLMCPC starts, it spawns external MCP servers as child processes using <code>fork()</code> and <code>pipe()</code>:</p> <pre><code>// Parent process keeps stdin_pipe[1] and stdout_pipe[0]\n// Child process uses stdin_pipe[0] for input, stdout_pipe[1] for output\n</code></pre>"},{"location":"protocol/#2-initialization-handshake","title":"2. Initialization Handshake","text":"<p>Request (Client \u2192 Server): <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"id\": 1,\n  \"method\": \"initialize\",\n  \"params\": {\n    \"protocolVersion\": \"2024-11-05\",\n    \"clientInfo\": {\n      \"name\": \"cpp-mcp-client\",\n      \"version\": \"1.0.0\"\n    },\n    \"capabilities\": {}\n  }\n}\n</code></pre></p> <p>Response (Server \u2192 Client): <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"id\": 1,\n  \"result\": {\n    \"protocolVersion\": \"2024-11-05\",\n    \"serverInfo\": {\n      \"name\": \"filesystem-server\",\n      \"version\": \"1.0.0\"\n    },\n    \"capabilities\": {\n      \"tools\": {}\n    }\n  }\n}\n</code></pre></p> <p>Notification (Client \u2192 Server): <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"method\": \"notifications/initialized\",\n  \"params\": {}\n}\n</code></pre></p>"},{"location":"protocol/#3-tool-discovery","title":"3. Tool Discovery","text":"<p>Request: <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"id\": 2,\n  \"method\": \"tools/list\",\n  \"params\": {}\n}\n</code></pre></p> <p>Response: <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"id\": 2,\n  \"result\": {\n    \"tools\": [\n      {\n        \"name\": \"read_file\",\n        \"description\": \"Read the contents of a file\",\n        \"inputSchema\": {\n          \"type\": \"object\",\n          \"properties\": {\n            \"path\": {\n              \"type\": \"string\",\n              \"description\": \"Path to the file\"\n            }\n          },\n          \"required\": [\"path\"]\n        }\n      }\n    ]\n  }\n}\n</code></pre></p>"},{"location":"protocol/#4-tool-execution","title":"4. Tool Execution","text":"<p>Request: <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"id\": 3,\n  \"method\": \"tools/call\",\n  \"params\": {\n    \"name\": \"read_file\",\n    \"arguments\": {\n      \"path\": \"/home/user/example.txt\"\n    }\n  }\n}\n</code></pre></p> <p>Response: <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"id\": 3,\n  \"result\": {\n    \"content\": [\n      {\n        \"type\": \"text\",\n        \"text\": \"File contents here...\"\n      }\n    ]\n  }\n}\n</code></pre></p>"},{"location":"protocol/#llm-integration","title":"LLM Integration","text":""},{"location":"protocol/#tool-schema-transformation","title":"Tool Schema Transformation","text":"<p>MCP tools are transformed into LLM-compatible function schemas. Here's how OLLMCPC does it:</p>"},{"location":"protocol/#from-mcp-format","title":"From MCP Format","text":"<pre><code>{\n  \"name\": \"read_file\",\n  \"description\": \"Read a file from the filesystem\",\n  \"inputSchema\": {\n    \"type\": \"object\",\n    \"properties\": {\n      \"path\": {\"type\": \"string\"}\n    }\n  }\n}\n</code></pre>"},{"location":"protocol/#to-ollamaopenai-format","title":"To Ollama/OpenAI Format","text":"<pre><code>{\n  \"type\": \"function\",\n  \"function\": {\n    \"name\": \"filesystem_read_file\",\n    \"description\": \"Read a file from the filesystem\",\n    \"parameters\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"path\": {\"type\": \"string\"}\n      }\n    }\n  }\n}\n</code></pre> <p>Note: OLLMCPC prefixes tool names with the server name (e.g., <code>filesystem_read_file</code>) to avoid naming collisions when multiple servers provide similar tools.</p>"},{"location":"protocol/#complete-llm-request","title":"Complete LLM Request","text":"<p>When you send a message to the LLM, OLLMCPC constructs a request like this:</p> <pre><code>{\n  \"model\": \"functiongemma\",\n  \"messages\": [\n    {\n      \"role\": \"system\",\n      \"content\": \"You are a powerful terminal assistant. You can use tools to interact with the system...\"\n    },\n    {\n      \"role\": \"user\",\n      \"content\": \"What files are in my home directory?\"\n    }\n  ],\n  \"tools\": [\n    {\n      \"type\": \"function\",\n      \"function\": {\n        \"name\": \"filesystem_list_directory\",\n        \"description\": \"List files in a directory\",\n        \"parameters\": {\n          \"type\": \"object\",\n          \"properties\": {\n            \"path\": {\"type\": \"string\"}\n          }\n        }\n      }\n    },\n    {\n      \"type\": \"function\",\n      \"function\": {\n        \"name\": \"os_assistant_run_shell_command\",\n        \"description\": \"Execute a shell command\",\n        \"parameters\": {\n          \"type\": \"object\",\n          \"properties\": {\n            \"command\": {\"type\": \"string\"}\n          }\n        }\n      }\n    }\n  ],\n  \"stream\": false\n}\n</code></pre>"},{"location":"protocol/#llm-response-with-tool-call","title":"LLM Response with Tool Call","text":"<p>The LLM responds with a tool call request:</p> <pre><code>{\n  \"message\": {\n    \"role\": \"assistant\",\n    \"content\": \"\",\n    \"tool_calls\": [\n      {\n        \"id\": \"call_abc123\",\n        \"type\": \"function\",\n        \"function\": {\n          \"name\": \"filesystem_list_directory\",\n          \"arguments\": \"{\\\"path\\\": \\\"/home/user\\\"}\"\n        }\n      }\n    ]\n  }\n}\n</code></pre>"},{"location":"protocol/#the-conversation-loop","title":"The Conversation Loop","text":"<p>OLLMCPC implements an intelligent multi-turn conversation loop:</p>"},{"location":"protocol/#step-by-step-flow","title":"Step-by-Step Flow","text":"<ol> <li>User Input: User sends a message</li> <li>LLM Analysis: Message + conversation history + available tools \u2192 LLM</li> <li>Tool Call Detection: Parse LLM response for <code>tool_calls</code></li> <li>Human-in-the-Loop (HIL): If enabled, ask user for approval</li> <li>Tool Execution: Route to correct MCP server, execute tool</li> <li>Result Injection: Add tool result to conversation history</li> <li>Synthesis: LLM processes the tool result and generates a human-readable response</li> <li>Loop: Repeat if LLM requests more tools (up to <code>loop_limit</code>)</li> </ol>"},{"location":"protocol/#example-conversation-history","title":"Example Conversation History","text":"<pre><code>[\n  {\n    \"role\": \"system\",\n    \"content\": \"You are a powerful terminal assistant...\"\n  },\n  {\n    \"role\": \"user\",\n    \"content\": \"What's my CPU usage?\"\n  },\n  {\n    \"role\": \"assistant\",\n    \"content\": \"\",\n    \"tool_calls\": [{\"function\": {\"name\": \"os_assistant_cpu_usage\", \"arguments\": \"{}\"}}]\n  },\n  {\n    \"role\": \"user\",\n    \"content\": \"Action: Tool [cpu_usage] executed.\\nResult: CPU: 23.4%\\nConstraint: Please summarize this data clearly...\"\n  },\n  {\n    \"role\": \"assistant\",\n    \"content\": \"Your current CPU usage is 23.4%, which indicates a light system load.\"\n  }\n]\n</code></pre>"},{"location":"protocol/#implementation-details","title":"Implementation Details","text":""},{"location":"protocol/#pipe-communication","title":"Pipe Communication","text":"<p>OLLMCPC uses UNIX pipes for stdio communication with MCP servers:</p> <pre><code>// Write request to server's stdin\nstd::string json_msg = json::obj(msg) + \"\\n\";\nwrite(stdin_pipe[1], json_msg.c_str(), json_msg.length());\n\n// Read response from server's stdout\nchar c;\nstd::string line;\nwhile (read(stdout_pipe[0], &amp;c, 1) &gt; 0) {\n    if (c == '\\n') break;\n    line += c;\n}\n</code></pre>"},{"location":"protocol/#handling-non-json-output","title":"Handling Non-JSON Output","text":"<p>Many MCP servers (especially those run via <code>npx</code>) output logs or warnings to stdout. OLLMCPC filters these:</p> <pre><code>// Skip lines that don't start with '{'\nif (line[0] != '{') {\n    Logger::debug(\"junk: \" + line);\n    continue;\n}\n</code></pre>"},{"location":"protocol/#loop-prevention","title":"Loop Prevention","text":"<p>To prevent infinite loops, OLLMCPC tracks tool signatures within a single turn:</p> <pre><code>std::string sig = tool_name + \":\" + tool_args;\nif (turn_tool_signatures.count(sig)) {\n    // Already called this exact tool with these args\n    break;\n}\nturn_tool_signatures.insert(sig);\n</code></pre>"},{"location":"protocol/#security-considerations","title":"Security Considerations","text":""},{"location":"protocol/#human-in-the-loop-hil","title":"Human-in-the-Loop (HIL)","text":"<p>Before executing any tool, OLLMCPC can prompt for user approval:</p> <pre><code>\u256d\u2500 GUARD - ACTION PENDING \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 Action:  run_shell_command                  \u2502\n\u2502 Inputs:  {\"command\": \"rm -rf /tmp/cache\"}   \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\nExecute? ([y]/n):\n</code></pre>"},{"location":"protocol/#sudo-handling","title":"Sudo Handling","text":"<p>When a tool requests <code>sudo</code>, OLLMCPC: 1. Detects the <code>sudo</code> prefix in the command 2. Prompts the user for their password (with hidden input) 3. Rewrites the command to use <code>echo password | sudo -S</code></p>"},{"location":"protocol/#argument-sanitization","title":"Argument Sanitization","text":"<p>Tool arguments are compacted to prevent newline injection:</p> <pre><code>for (char c : tool_args) {\n    if (c == '\\n' || c == '\\r' || c == '\\t') \n        compacted_args += ' ';\n    else \n        compacted_args += c;\n}\n</code></pre>"},{"location":"protocol/#references","title":"References","text":"<ul> <li>Model Context Protocol Specification</li> <li>JSON-RPC 2.0 Specification</li> <li>Source: <code>src/src/mcp/server_proxy.cpp</code></li> <li>Source: <code>src/src/app/interactive.cpp</code></li> <li>Source: <code>src/src/llm/ollama.cpp</code></li> </ul>"},{"location":"quickstart/","title":"Quick Start Guide","text":"<p>Get up and running with OLLMCPC in 60 seconds.</p>"},{"location":"quickstart/#1-run-the-setup-script","title":"1. Run the Setup Script","text":"<p>This handles dependencies and builds the project. <pre><code>chmod +x scripts/setup.sh\n./scripts/setup.sh\n</code></pre></p>"},{"location":"quickstart/#2-configure-your-models","title":"2. Configure Your Models","text":"<p>Run the wizard to set your preferred provider. <pre><code>./build/ollmcpc config\n</code></pre></p>"},{"location":"quickstart/#3-pull-a-model-if-using-ollama","title":"3. Pull a Model (If using Ollama)","text":"<pre><code>ollama pull functiongemma\n</code></pre>"},{"location":"quickstart/#4-start-chatting","title":"4. Start Chatting!","text":"<pre><code>./build/ollmcpc serve\n</code></pre>"},{"location":"quickstart/#5-try-a-tool","title":"5. Try a Tool","text":"<p>Ask something like:</p> <p>\"What is my current CPU usage and what are the top 5 largest files in my home directory?\"</p> <p>Accept the HIL prompts and watch your AI interact with your system!</p>"},{"location":"troubleshooting/","title":"Troubleshooting","text":"<p>Common issues and how to solve them.</p>"},{"location":"troubleshooting/#installation-issues","title":"Installation Issues","text":""},{"location":"troubleshooting/#missing-libcurl","title":"Missing <code>libcurl</code>","text":"<p>If <code>cmake</code> fails with a \"Could not find CURL\" error: <pre><code>sudo apt-get install libcurl4-openssl-dev\n</code></pre></p>"},{"location":"troubleshooting/#compiler-too-old","title":"Compiler too old","text":"<p>OLLMCPC requires C++17. If your compiler is too old: <pre><code>sudo apt-get install gcc-11 g++-11\n</code></pre> Then run cmake with: <code>CXX=g++-11 cmake ..</code></p>"},{"location":"troubleshooting/#runtime-issues","title":"Runtime Issues","text":""},{"location":"troubleshooting/#ollama-connection-refused-couldnt-connect-to-server","title":"Ollama Connection Refused / Couldn't Connect to Server","text":"<p>If you see: <pre><code>curl_easy_perform() failed: Couldn't connect to server\n</code></pre> or <pre><code>Ollama Connection Refused\n</code></pre></p> <p>This means Ollama isn't running. Start it with: <pre><code>ollama serve\n</code></pre></p> <p>To run it in the background: <pre><code>nohup ollama serve &gt; /tmp/ollama.log 2&gt;&amp;1 &amp;\n</code></pre></p>"},{"location":"troubleshooting/#architecture-mismatch-qemu-errors","title":"Architecture Mismatch (qemu errors)","text":"<p>If you see errors like: <pre><code>qemu-x86_64: Could not open '/lib64/ld-linux-x86-64.so.2': No such file or directory\n</code></pre></p> <p>This means the binaries were compiled for a different CPU architecture (x86_64 vs ARM64). You must rebuild on your own machine: <pre><code>cd build\nrm -rf *\ncmake ..\nmake -j$(nproc)\nsudo cp ollmcpc mcp_server /usr/local/bin/\n</code></pre></p>"},{"location":"troubleshooting/#server-error-npx-not-found","title":"Server Error: \"npx not found\"","text":"<p>External servers often require Node.js. Ensure it's installed and in your PATH: <pre><code>node -v\nnpm -v\n</code></pre></p>"},{"location":"troubleshooting/#tool-execution-fails","title":"Tool Execution Fails","text":"<p>Check <code>debug.log</code> in the root directory. It contains all JSON-RPC traffic between OLLMCPC and the MCP servers.</p> <pre><code>tail -f debug.log\n</code></pre>"},{"location":"troubleshooting/#failed-to-exec-server-for-os-assistant","title":"\"Failed to exec server\" for os-assistant","text":"<p>If you see this error on startup: <pre><code>Failed to exec server\nFailed to initialize\n</code></pre></p> <p>This means the internal <code>mcp_server</code> binary cannot be found. Solutions:</p> <ol> <li> <p>If installed via setup script: The binary should be in <code>/usr/local/bin/mcp_server</code>. Verify with:    <pre><code>which mcp_server\n</code></pre></p> </li> <li> <p>If running from build directory: Use the full path:    <pre><code>./build/ollmcpc serve\n</code></pre>    Or ensure <code>mcp_server</code> is in your PATH.</p> </li> <li> <p>Rebuild and reinstall:    <pre><code>cd build &amp;&amp; make -j$(nproc)\nsudo cp mcp_server /usr/local/bin/\nsudo cp ollmcpc /usr/local/bin/\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting/#protocol-issues","title":"Protocol Issues","text":""},{"location":"troubleshooting/#method-not-found","title":"\"Method not found\"","text":"<p>This usually means the MCP server doesn't support a specific feature. Ensure you are using a compatible version of the server.</p>"},{"location":"troubleshooting/#hil-prompt-not-appearing","title":"HIL Prompt not appearing","text":"<p>Check your configuration (<code>~/.ollmcpc.json</code>) and ensure <code>\"human_in_loop\": true</code> is set. Note that \"Manual Mode\" bypasses HIL prompts as you are already manually selecting tools.</p>"},{"location":"advanced/commands/","title":"Commands &amp; Toggling","text":"<p>OLLMCPC provides several internal commands to manage your session in real-time.</p>"},{"location":"advanced/commands/#slash-commands","title":"Slash Commands","text":"<p>In the interactive chat, any line starting with <code>/</code> is treated as a command rather than a prompt for the LLM.</p>"},{"location":"advanced/commands/#help","title":"<code>/help</code>","text":"<p>Displays a list of all available slash commands and their descriptions.</p>"},{"location":"advanced/commands/#servers","title":"<code>/servers</code>","text":"<p>Lists all configured MCP servers and their current status: *   Active: The server is running and communicating. *   Disabled: The server is configured but turned off by the user. *   Error: The server failed to start or lost connection.</p>"},{"location":"advanced/commands/#toggle-name","title":"<code>/toggle &lt;name&gt;</code>","text":"<p>Enables or disables a specific server by its name. *   Example: <code>/toggle filesystem</code> *   Effect: If the server was active, it will be shut down. If it was disabled, it will be launched and initialized.</p>"},{"location":"advanced/commands/#exit","title":"<code>/exit</code>","text":"<p>Safely shuts down all MCP servers and exits the application.</p>"},{"location":"advanced/commands/#human-in-the-loop-hil","title":"Human-in-the-Loop (HIL)","text":"<p>Safety is a core principle of OLLMCPC. When HIL is enabled (default), the client will intercept every tool execution request.</p>"},{"location":"advanced/commands/#approval-prompt","title":"Approval Prompt","text":"<p>When the LLM wants to run a tool, you will see a colored box showing: *   The tool name. *   The arguments the LLM wants to pass.</p> <p>You can then: *   Press Enter: Approve the execution. *   Type 'no': Deny the execution (the LLM will receive a \"User denied execution\" error). *   Type 'edit': (Planned) Modify the arguments before execution.</p>"},{"location":"advanced/custom_servers/","title":"Custom MCP Servers","text":"<p>You can easily extend OLLMCPC by adding your own MCP servers.</p>"},{"location":"advanced/custom_servers/#popular-mcp-servers","title":"Popular MCP Servers","text":"<p>The Model Context Protocol has a growing ecosystem. You can find many servers on GitHub or via NPM.</p>"},{"location":"advanced/custom_servers/#examples","title":"Examples:","text":"<ul> <li>PostgreSQL: <code>npx -y @modelcontextprotocol/server-postgres postgres://...</code></li> <li>Google Maps: <code>npx -y @modelcontextprotocol/server-google-maps</code></li> <li>GitHub: <code>npx -y @modelcontextprotocol/server-github</code></li> </ul>"},{"location":"advanced/custom_servers/#adding-to-ollmcpc","title":"Adding to OLLMCPC","text":"<p>To add a new server, edit your <code>~/.ollmcpc.json</code> and add a new entry to the <code>servers</code> array.</p> <pre><code>{\n  \"name\": \"my-sql-server\",\n  \"command\": [\"npx\", \"-y\", \"@modelcontextprotocol/server-postgres\", \"postgresql://localhost/mydb\"],\n  \"enabled\": true\n}\n</code></pre>"},{"location":"advanced/custom_servers/#creating-your-own-server","title":"Creating Your Own Server","text":"<p>If you want to create a tool specifically for your workflow:</p> <ol> <li>Choose a Language: Node.js, Python, and Go have excellent MCP SDKs.</li> <li>Implement JSON-RPC: Your server must speak JSON-RPC 2.0 over <code>stdin</code>/<code>stdout</code>.</li> <li>Define Tools: Expose the tools your LLM needs.</li> <li>Register it: Add it to OLLMCPC configuration.</li> </ol>"},{"location":"advanced/custom_servers/#tool-naming","title":"Tool Naming","text":"<p>OLLMCPC automatically prefixes your tools with the server name to avoid collisions. *   Server: <code>filesystem</code> *   Tool: <code>read_file</code> *   LLM sees: <code>filesystem_read_file</code></p>"},{"location":"components/app/","title":"Application Layer","text":"<p>The Application layer manages the user interaction and high-level orchestration of the system.</p>"},{"location":"components/app/#entry-point-maincpp","title":"Entry Point (<code>main.cpp</code>)","text":"<p>Located in <code>src/main.cpp</code>. It uses a custom CLI parser to route commands: *   <code>serve</code>: Starts the interactive chat session. *   <code>config</code>: Starts the configuration wizard. *   <code>list</code>: Lists available models from the configured providers.</p>"},{"location":"components/app/#interactive-mode-interactivecpp","title":"Interactive Mode (<code>interactive.cpp</code>)","text":"<p>The \"Brain\" of the UI, found in <code>src/app/interactive.cpp</code>.</p>"},{"location":"components/app/#responsibilities","title":"Responsibilities:","text":"<ul> <li>Chat Loop: Keeps the conversation going until the user exits.</li> <li>Slash Commands: Processes commands like <code>/help</code>, <code>/servers</code>, and <code>/toggle</code>.</li> <li>HIL Logic: Implements the Human-in-the-Loop check before any tool is executed.</li> <li>Terminal Styling: Uses <code>utils/terminal.h</code> to provide a rich, colorized output.</li> </ul>"},{"location":"components/app/#configuration-manager-configcpp","title":"Configuration Manager (<code>config.cpp</code>)","text":"<p>Found in <code>src/app/config.cpp</code>.</p>"},{"location":"components/app/#features","title":"Features:","text":"<ul> <li>JSON Persistence: Saves and loads settings from <code>~/.ollmcpc.json</code>.</li> <li>Interactive Setup: Provides a terminal-based UI for initial configuration.</li> <li>Default Injection: Ensures healthy defaults are present if the config file is missing or corrupted.</li> </ul>"},{"location":"components/app/#key-files","title":"Key Files:","text":"<ul> <li><code>src/include/app/interactive.h</code>: UI definitions.</li> <li><code>src/include/app/config.h</code>: Configuration structures.</li> <li><code>src/src/app/interactive.cpp</code>: Implementation of the UI logic.</li> <li><code>src/src/app/config.cpp</code>: Implementation of persistence logic.</li> </ul>"},{"location":"components/llm/","title":"LLM Providers","text":"<p>OLLMCPC uses a Strategy Pattern to support multiple LLM backends. All providers inherit from the <code>LLMProvider</code> base class.</p>"},{"location":"components/llm/#base-class-llm_providerh","title":"Base Class (<code>llm_provider.h</code>)","text":"<p>Defines the interface for communication: *   <code>chat(messages, tools)</code>: Sends a prompt and available tools to the LLM. *   <code>list_models()</code>: Returns a list of supported models.</p>"},{"location":"components/llm/#ollama-provider-ollama_providercpp","title":"Ollama Provider (<code>ollama_provider.cpp</code>)","text":"<p>Connects to a local Ollama instance (usually at <code>http://localhost:11434</code>).</p>"},{"location":"components/llm/#features","title":"Features:","text":"<ul> <li>Standard chat-completion API.</li> <li>Native tool support (functions).</li> <li>Streaming responses.</li> </ul>"},{"location":"components/llm/#gemini-provider-gemini_providercpp","title":"Gemini Provider (<code>gemini_provider.cpp</code>)","text":"<p>Connects to Google's Gemini API.</p>"},{"location":"components/llm/#features_1","title":"Features:","text":"<ul> <li>High latency but powerful reasoning.</li> <li>Strict Tool-Calling schemas.</li> <li>Requires an API Key.</li> </ul>"},{"location":"components/llm/#manual-provider-manual_providercpp","title":"Manual Provider (<code>manual_provider.cpp</code>)","text":"<p>A special \"debug\" provider that doesn't use an LLM.</p>"},{"location":"components/llm/#purpose","title":"Purpose:","text":"<ul> <li>Allows the user to manually select tools from a menu.</li> <li>Perfect for testing if a new MCP tool is working correctly without spending tokens or waiting for local inference.</li> </ul>"},{"location":"components/llm/#key-files","title":"Key Files:","text":"<ul> <li><code>src/include/llm/llm_provider.h</code>: Abstract base class.</li> <li><code>src/src/llm/ollama_provider.cpp</code>: Ollama implementation.</li> <li><code>src/src/llm/gemini_provider.cpp</code>: Gemini implementation.</li> <li><code>src/src/llm/manual_provider.cpp</code>: Interactive tool selector.</li> </ul>"},{"location":"components/mcp/","title":"MCP Backend","text":"<p>The MCP Backend is the core of OLLMCPC's power, implementing the Model Context Protocol communication.</p>"},{"location":"components/mcp/#mcp-client-clientcpp","title":"MCP Client (<code>client.cpp</code>)","text":"<p>The <code>MCPClient</code> manages the lifecycle of all connected servers.</p>"},{"location":"components/mcp/#functions","title":"Functions:","text":"<ul> <li>Initialization: Sends the <code>initialize</code> request to all servers.</li> <li>Tool Aggregation: Merges tools from all servers into a single list for the LLM.</li> <li>Routing: When a tool called <code>filesystem_read_file</code> is requested, it knows to strip the <code>filesystem_</code> prefix and send the <code>read_file</code> request to the correct server.</li> </ul>"},{"location":"components/mcp/#server-proxy-server_proxycpp","title":"Server Proxy (<code>server_proxy.cpp</code>)","text":"<p>Handles the low-level details of running an external MCP server.</p>"},{"location":"components/mcp/#implementation","title":"Implementation:","text":"<ul> <li>Pipe Communication: Uses <code>dup2</code> and <code>pipe</code> to capture <code>stdin</code> and <code>stdout</code> of the child process.</li> <li>JSON-RPC 2.0: Implements the standard JSON-RPC protocol over the pipes.</li> <li>Async Reading: Uses threads to read output from the server so the main client stays responsive.</li> </ul>"},{"location":"components/mcp/#internal-server-mcp_server-target","title":"Internal Server (<code>mcp_server</code> target)","text":"<p>A standalone binary built from <code>src/mcp/server.cpp</code>. It provides the \"OS Assistant\" tools.</p>"},{"location":"components/mcp/#key-tools","title":"Key Tools:","text":"<ul> <li><code>execute_shell</code>: Run arbitrary commands safely.</li> <li><code>cpu_usage</code>: Monitor system load.</li> <li><code>process_list</code>: See what's running.</li> </ul>"},{"location":"components/mcp/#key-files","title":"Key Files:","text":"<ul> <li><code>src/include/mcp/client.h</code>: The MCP registry and manager.</li> <li><code>src/include/mcp/server_proxy.h</code>: External process bridge.</li> <li><code>src/src/mcp/client.cpp</code>: Logic for cross-server coordination.</li> <li><code>src/src/mcp/server_proxy.cpp</code>: Logic for stdio/pipe communication.</li> </ul>"},{"location":"components/utils/","title":"Utilities","text":"<p>OLLMCPC includes a set of lightweight utilities to minimize external dependencies.</p>"},{"location":"components/utils/#curl-wrapper-curl_utilscpp","title":"CURL Wrapper (<code>curl_utils.cpp</code>)","text":"<p>A thin, thread-safe wrapper around <code>libcurl</code> for making HTTP requests to Ollama and Gemini APIs.</p>"},{"location":"components/utils/#terminal-styling-terminalh","title":"Terminal Styling (<code>terminal.h</code>)","text":"<p>Provides macros and functions for: *   ANSI color output. *   Box drawing. *   Progress indicators (simplified). *   Bold/Italic/Underline text.</p>"},{"location":"components/utils/#json-handling-json_utilsh","title":"JSON Handling (<code>json_utils.h</code>)","text":"<p>While the project uses <code>nlohmann/json</code> for complex parsing, the <code>utils</code> folder contains helper functions for: *   Safe value extraction. *   Pretty-printing JSON logic specifically for the terminal.</p>"},{"location":"components/utils/#logging-loggerh","title":"Logging (<code>logger.h</code>)","text":"<p>A simple logger that outputs to <code>debug.log</code>. It can be configured for different levels: *   <code>DEBUG</code>: Protocol messages, raw JSON-RPC. *   <code>INFO</code>: Normal operation. *   <code>ERROR</code>: Connection failures or protocol violations.</p>"},{"location":"components/utils/#key-files","title":"Key Files:","text":"<ul> <li><code>src/include/utils/curl_utils.h</code>: Network communication.</li> <li><code>src/include/utils/terminal.h</code>: Aesthetic output tools.</li> <li><code>src/include/utils/logger.h</code>: Debugging and monitoring.</li> </ul>"}]}